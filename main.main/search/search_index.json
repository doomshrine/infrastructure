{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quickstart","text":""},{"location":"#pre-requisites","title":"Pre-requisites","text":"<p>To run the following examples, you will need to have the following installed:</p> <ul> <li>just</li> <li>packer</li> <li>python3 (&gt;=3.9)</li> <li>pip</li> </ul> <p>You will also need to have the Proxmox VE installed on a VM or bare metal server.</p>"},{"location":"#configuration","title":"Configuration","text":"<p>Run <code>just config</code> to generate configuration files. This will prompt you for the following:</p> <ul> <li>Proxmox Address - The IP address of your Proxmox VE server. No protocol, port, or trailing slash.</li> <li>Proxmox Node - The name of the Proxmox node to use for building images.</li> <li>Proxmox User - The username to use for authenticating with the Proxmox API.</li> <li>Proxmox Password - The password to use for authenticating with the Proxmox API.</li> <li>Storage Pool - The name of the Proxmox storage pool to use for building images.</li> <li>Prefix - The prefix to use for the names of the VM templates created by Packer.</li> </ul> <p>The new configuration files will be saved to:</p> <ul> <li><code>~/.config/packer/variables.pkrvars.hcl</code> - Packer variables file.</li> <li><code>~/.config/terraform/terraform.tfvars</code> - Terraform variables file.</li> </ul> <p>You can edit these files directly if you need to change any of the values.</p>"},{"location":"#building-templates","title":"Building templates","text":"<p>Run <code>just templates</code> to build the following templates:</p> <ul> <li><code>rocky-9</code> - Rocky 9.x minimal, the minor version is picked up automatically from the Release page.</li> <li><code>ubuntu-22.04</code> - Ubuntu 22.04 LTS Server - the patch version is picked up automatically from the Release page.</li> <li><code>vyos</code> - VyOS nightly build. Installation is quite sketchy, but it works. Always uses the latest nightly build.</li> </ul>"},{"location":"#deploying-k3s-ha-cluster-with-cilium-cni-and-kured","title":"Deploying K3s HA Cluster with Cilium CNI and Kured","text":"<ol> <li>Run <code>just k3s</code>. It will build the template, start the VMs, initialize them and finally install K3s.<ul> <li>You will be asked for some inputs:<ol> <li>When bringing up infrastructure, you need to pass number of nodes. Answer <code>5</code>, so there will be no modifications to other scripts needed.</li> <li>You will need inventory file. This can be generated. Just answer <code>Y</code> when asked for generating it, and provide all necessary information.</li> <li>Finally you will be asked which host group should be used. If you generated new inventory file, then type <code>generated</code>. Otherwise use the file you created manually.</li> </ol> </li> </ul> </li> <li>Run <code>just k3s_cilium</code> to install Cilium CNI and Hubble.</li> <li>Run <code>just k3s_kured \"&lt;SHOUTRR URL&gt;\"</code> (providing valid Shoutrrr notification URL), so the Kured will be installed.</li> </ol>"},{"location":"ansible-playbooks/docker/","title":"Docker","text":"<p>This playbook provides an automated installation of Docker using the <code>geerlingguy.docker</code> role. It also includes the configuration and execution of basic containers.</p>"},{"location":"ansible-playbooks/docker/#host-groups","title":"Host groups","text":"<p>To run this playbook, you need to define a host group named <code>dockerhost</code>. The playbook will only be executed on the hosts specified within this host group.</p>"},{"location":"ansible-playbooks/docker/#variables","title":"Variables","text":""},{"location":"ansible-playbooks/docker/#shoutrrr_notifications_url","title":"<code>shoutrrr_notifications_url</code>","text":"<p>This variable specifies the Shoutrrr notifications URL, which is required for Watchtower deployment. For the URL format and additional information, please refer to the shoutrrr documentation.</p>"},{"location":"ansible-playbooks/ingress/","title":"Ingress","text":""},{"location":"ansible-playbooks/ingress/#host-groups","title":"Host groups","text":""},{"location":"ansible-playbooks/ingress/#ingress_1","title":"<code>ingress</code>","text":"<p>This host group is used for configuring and managing the Ingress functionality.</p>"},{"location":"ansible-playbooks/ingress/#variables","title":"Variables","text":""},{"location":"ansible-playbooks/ingress/#woodpecker_secret","title":"<code>woodpecker_secret</code>","text":"<p>The <code>woodpecker_secret</code> variable contains a secret value required for the Ingress configuration.</p>"},{"location":"ansible-playbooks/ingress/#woodpecker_github_client","title":"<code>woodpecker_github_client</code>","text":"<p>This variable holds the GitHub client information necessary for the Ingress configuration.</p>"},{"location":"ansible-playbooks/ingress/#woodpecker_github_secret","title":"<code>woodpecker_github_secret</code>","text":"<p>The <code>woodpecker_github_secret</code> variable stores the GitHub secret value used in the Ingress configuration.</p>"},{"location":"ansible-playbooks/jenkins/","title":"Jenkins","text":""},{"location":"ansible-playbooks/jenkins/#host-groups","title":"Host groups","text":""},{"location":"ansible-playbooks/jenkins/#jenkins_1","title":"<code>jenkins</code>","text":"<p>The <code>jenkins</code> host group is used for managing the Jenkins infrastructure.</p>"},{"location":"ansible-playbooks/jenkins/#variables","title":"Variables","text":""},{"location":"ansible-playbooks/load-balancer/","title":"Load balancer for K8s API Server","text":""},{"location":"ansible-playbooks/load-balancer/#host-groups","title":"Host groups","text":""},{"location":"ansible-playbooks/load-balancer/#loadbalancer","title":"<code>loadbalancer</code>","text":"<p>The <code>loadbalancer</code> host group contains the hosts responsible for load balancing the Kubernetes API server.</p>"},{"location":"ansible-playbooks/load-balancer/#variables","title":"Variables","text":""},{"location":"ansible-playbooks/load-balancer/#subnet","title":"<code>subnet</code>","text":"<p>The <code>subnet</code> variable specifies the subnet configuration for the load balancer.</p>"},{"location":"ansible-playbooks/load-balancer/#gateway","title":"<code>gateway</code>","text":"<p>The <code>gateway</code> variable holds the gateway information required for the load balancer configuration.</p>"},{"location":"ansible-playbooks/load-balancer/#interface","title":"<code>interface</code>","text":"<p>The <code>interface</code> variable defines the network interface used by the load balancer.</p>"},{"location":"ansible-playbooks/load-balancer/#netmask","title":"<code>netmask</code>","text":"<p>The <code>netmask</code> variable stores the netmask value used in the load balancer configuration.</p>"},{"location":"ansible-playbooks/load-balancer/#controlplane1-controlplane2-controlplane3","title":"<code>controlplane1</code>, <code>controlplane2</code>, <code>controlplane3</code>","text":"<p>These variables represent the control plane nodes that the load balancer will direct traffic to.</p>"},{"location":"ansible-playbooks/new-host/","title":"New host","text":""},{"location":"ansible-playbooks/new-host/#host-groups","title":"Host groups","text":""},{"location":"ansible-playbooks/new-host/#new","title":"<code>new</code>","text":"<p>The <code>new</code> host group is used for managing new hosts that are added to the system.</p>"},{"location":"ansible-playbooks/new-host/#variables","title":"Variables","text":""},{"location":"ansible-playbooks/new-host/#new_user","title":"<code>new_user</code>","text":"<p>The <code>new_user</code> variable specifies the user account to be created on the new host.</p>"},{"location":"ansible-playbooks/new-host/#hostname","title":"<code>hostname</code>","text":"<p>The <code>hostname</code> variable contains the desired hostname for the new host.</p>"},{"location":"ansible-playbooks/new-host/#tailscale_key","title":"<code>tailscale_key</code>","text":"<p>The <code>tailscale_key</code> variable holds the Tailscale key required for connecting the new host to the network.</p>"},{"location":"ansible-playbooks/new-host/#static_route","title":"<code>static_route</code>","text":"<p>This variable defines a static route configuration. Note that it is a conditional variable, and if set, the variables <code>subnet</code>, <code>gateway</code>, and <code>interface</code> must also be set.</p>"},{"location":"ansible-playbooks/new-host/#subnet","title":"<code>subnet</code>","text":"<p>The <code>subnet</code> variable specifies the subnet configuration for the new host.</p>"},{"location":"ansible-playbooks/new-host/#gateway","title":"<code>gateway</code>","text":"<p>The <code>gateway</code> variable holds the gateway information required for the new host configuration.</p>"},{"location":"ansible-playbooks/new-host/#interface","title":"<code>interface</code>","text":"<p>The <code>interface</code> variable defines the network interface used by the new host.</p>"},{"location":"ansible-playbooks/new-host/#ipv4","title":"<code>ipv4</code>","text":"<p>The <code>ipv4</code> variable contains the IPv4 address for the new host.</p>"},{"location":"ansible-playbooks/new-host/#netmask","title":"<code>netmask</code>","text":"<p>The <code>netmask</code> variable stores the netmask value used in the new host configuration.</p>"},{"location":"ansible-playbooks/new-host/#disk_device","title":"<code>disk_device</code>","text":"<p>The <code>disk_device</code> variable specifies the disk device to be used on the new host.</p>"},{"location":"ansible-playbooks/new-host/#partition_number","title":"<code>partition_number</code>","text":"<p>This variable represents the partition number on the specified <code>disk_device</code>. It is a group variable and must be set along with <code>partition_size</code> if used.</p>"},{"location":"ansible-playbooks/new-host/#partition_size","title":"<code>partition_size</code>","text":"<p>The <code>partition_size</code> variable defines the size of the partition on the specified <code>disk_device</code>. It is a group variable and must be set along with <code>partition_number</code> if used.</p>"},{"location":"ansible-playbooks/new-host/#skip_reboot","title":"<code>skip_reboot</code>","text":"<p>This variable indicates whether a manual reboot is required after the configuration. If set, a manual reboot is necessary.</p>"},{"location":"ansible-playbooks/pfsense/","title":"Pfsense","text":""},{"location":"ansible-playbooks/pfsense/#host-groups","title":"Host groups","text":""},{"location":"ansible-playbooks/pfsense/#pfsense_1","title":"<code>pfsense</code>","text":"<p>The <code>pfsense</code> host group is responsible for managing the Pfsense firewall system.</p>"},{"location":"ansible-playbooks/pfsense/#variables","title":"Variables","text":"<p>This section is missing information about the variables related to Pfsense. Please add the relevant variables here.</p>"},{"location":"ansible-playbooks/runner/","title":"Runner (Woodpecker Agent)","text":""},{"location":"ansible-playbooks/runner/#host-groups","title":"Host groups","text":""},{"location":"ansible-playbooks/runner/#runner","title":"<code>runner</code>","text":"<p>The <code>runner</code> host group is responsible for hosting the Woodpecker Agent, which is used for executing automation tasks.</p>"},{"location":"ansible-playbooks/runner/#variables","title":"Variables","text":""},{"location":"ansible-playbooks/runner/#max_concurrent_jobs","title":"<code>max_concurrent_jobs</code>","text":"<p>The <code>max_concurrent_jobs</code> variable defines the maximum number of concurrent jobs that the Woodpecker Agent can execute. It helps control the workload and resource allocation within the Woodpecker Agent environment. By setting this variable, you can ensure efficient utilization of system resources and manage the execution of automation tasks effectively.</p>"},{"location":"clusters/k3s-ha/","title":"High availability K3s with Cilium","text":"<p>\u26a0\ufe0f WARNING! Make sure to modify the variables in the script according to your environment before executing it!</p>"},{"location":"clusters/k3s-ha/#purpose","title":"Purpose","text":"<p>This file is a deployment script written in Bash. Its purpose is to automate the installation and configuration of various components in a Kubernetes cluster. The script makes use of several dependencies and performs tasks such as installing and configuring k3s, installing Cilium and Kured, and setting up Flux for GitOps deployment.</p>"},{"location":"clusters/k3s-ha/#components","title":"Components","text":"<p>The script consists of the following components:</p> <ol> <li> <p>Shebang and Variables: The script starts with a shebang specifying the interpreter to be used. It also defines several variables used throughout the script, such as <code>GITHUB_USER</code>, <code>INFRA_REPO</code>, <code>USER</code>, <code>SSH_KEY</code>, <code>SERVERS</code>, and <code>AGENTS</code>.</p> </li> <li> <p>Dependencies: This section checks if the required dependencies (<code>cilium</code>, <code>flux</code>, <code>kubectl</code>, <code>k3sup</code>) are installed. It invokes the <code>_check</code> function for each dependency to verify its presence.</p> </li> <li> <p>Uninstall k3s: The <code>prune</code> target is responsible for uninstalling k3s from the specified servers and agents. It uses SSH to execute the <code>k3s-agent-uninstall.sh</code> and <code>k3s-uninstall.sh</code> scripts on the respective machines.</p> </li> <li> <p>Install k3s: The <code>k3s</code> target installs k3s on the servers and joins them to form a cluster. It requires the dependencies to be installed beforehand. It invokes several internal functions, including <code>_k3s_pre</code>, <code>_k3s_server</code>, and <code>_k3s_agent</code>.</p> </li> <li> <p>Internal Functions:</p> <ul> <li><code>_k3s_pre</code>: Performs pre-installation checks and validations before installing k3s. It verifies the number of servers, the presence of the <code>USER</code> and <code>SSH_KEY</code> variables, and ensures a minimum of three servers for high availability.</li> <li><code>_k3s_server</code>: Installs k3s on the control plane server and generates the kubeconfig file. It sets up additional parameters such as the flannel backend, cluster CIDR, and disables certain components.</li> <li><code>_k3s_agent</code>: Joins the agent nodes to the k3s cluster by executing the <code>k3sup join</code> command for each agent.</li> <li><code>_k3s_is_ready</code> function waits for the k3s cluster to be ready by checking the readiness condition of the nodes using <code>kubectl wait</code>.</li> </ul> </li> <li> <p>Install Cilium: The <code>cilium</code> target installs and enables Cilium, a networking and security plugin, in the Kubernetes cluster. It requires the dependencies to be installed beforehand. It sets the <code>KUBECONFIG</code> environment variable to point to the generated kubeconfig file.</p> </li> <li> <p>Install Kured: The <code>kured</code> target installs and deploys Kured, a Kubernetes reboot daemon, in the cluster. It applies the <code>kured.yaml</code> manifest file using <code>kubectl</code>.</p> </li> <li> <p>Fetch Kured Manifest: The <code>_fetch_kured_manifest</code> function retrieves the latest release of Kured from GitHub and saves the manifest file locally in the <code>./generated/</code> directory.</p> </li> <li> <p>Configure Kured: The <code>kured_configure</code> function is responsible for modifying the fetched <code>kured.yaml</code> file to include additional configurations. It appends the <code>--notify-url</code> and <code>--reboot-sentinel-command</code> options based on the provided <code>TOKEN</code>.</p> </li> <li> <p>Install and Configure Flux: The <code>flux</code> target installs and configures Flux, a GitOps operator, in the cluster. It performs a pre-check using <code>flux check --pre</code> and then bootstraps Flux for GitHub integration. It requires the specified <code>PATH</code> to be the repository path where the Flux manifests are located.</p> </li> </ol>"},{"location":"clusters/k3s-ha/#usage","title":"Usage","text":"<p>To use this script, you can run it directly by executing the file or by invoking specific targets using the <code>just</code> command with the appropriate target name. For example:</p> <pre><code>$ just dependencies # Check dependencies\n$ just k3s # Install k3s and join servers and agents\n$ just cilium # Install Cilium\n$ just kured # Install Kured\n$ just flux ./path/to/repo # Install and configure Flux with the specified path\n</code></pre>"},{"location":"packer-templates/rocky-9/","title":"Rocky 9","text":""},{"location":"packer-templates/rocky-9/#purpose","title":"Purpose","text":"<p>The purpose of this Packer template is to create a virtual machine template for Rocky Linux 9.x on the Proxmox platform. Rocky Linux is an open-source enterprise operating system designed to be 100% bug-for-bug compatible with Red Hat Enterprise Linux. This template automates the creation of a virtual machine with the Rocky Linux 9.x operating system, pre-configured settings, and a customized kickstart file.</p>"},{"location":"packer-templates/rocky-9/#variables","title":"Variables","text":"<ul> <li> <p><code>name</code> (string): Specifies the name of the virtual machine template. The default value is set to \"rocky-9\".</p> </li> <li> <p><code>proxmox_addr</code> (string): Specifies the address of the Proxmox server.</p> </li> <li> <p><code>proxmox_node</code> (string): Specifies the Proxmox node where the virtual machine will be created.</p> </li> <li> <p><code>proxmox_username</code> (string): Specifies the username for authenticating with the Proxmox server.</p> </li> <li> <p><code>proxmox_password</code> (string): Specifies the password for authenticating with the Proxmox server.</p> </li> <li> <p><code>proxmox_token</code> (string): Specifies the Proxmox API token. The default value is obtained from the <code>PACKER_PVE_TOKEN</code> environment variable.</p> </li> <li> <p><code>vm_prefix</code> (string): Specifies a prefix for the virtual machine name.</p> </li> <li> <p><code>iso_storage</code> (string): Specifies the storage pool where the ISO image will be mounted.</p> </li> </ul>"},{"location":"packer-templates/ubuntu-22.04/","title":"Ubuntu 22.04 LTS","text":""},{"location":"packer-templates/ubuntu-22.04/#purpose","title":"Purpose","text":"<p>The purpose of this Packer template is to create a virtual machine template for Ubuntu 22.04.x Linux on the Proxmox platform. It automates the creation of a virtual machine with Ubuntu 22.04.x, pre-configured settings, and cloud-init for automated provisioning.</p>"},{"location":"packer-templates/ubuntu-22.04/#variables","title":"Variables","text":"<ul> <li> <p><code>name</code> (string): Specifies the name of the virtual machine template. The default value is set to \"ubuntu-22.04\".</p> </li> <li> <p><code>proxmox_addr</code> (string): Specifies the address of the Proxmox server.</p> </li> <li> <p><code>proxmox_node</code> (string): Specifies the Proxmox node where the virtual machine will be created.</p> </li> <li> <p><code>proxmox_username</code> (string): Specifies the username for authenticating with the Proxmox server.</p> </li> <li> <p><code>proxmox_password</code> (string): Specifies the password for authenticating with the Proxmox server.</p> </li> <li> <p><code>proxmox_token</code> (string): Specifies the Proxmox API token. The default value is obtained from the <code>PACKER_PVE_TOKEN</code> environment variable.</p> </li> <li> <p><code>vm_prefix</code> (string): Specifies a prefix for the virtual machine name.</p> </li> <li> <p><code>iso_storage</code> (string): Specifies the storage pool where the ISO image will be mounted.</p> </li> </ul>"},{"location":"packer-templates/vyos/","title":"VyOS","text":""},{"location":"packer-templates/vyos/#purpose","title":"Purpose","text":"<p>The purpose of this Packer template is to create a virtual machine template for VyOS Rolling Release on the Proxmox platform. It automates the creation of a virtual machine with VyOS Rolling Release, pre-configured settings, and an installation process using the VyOS command-line interface.</p>"},{"location":"packer-templates/vyos/#variables","title":"Variables","text":"<ul> <li> <p><code>name</code> (string): Specifies the name of the virtual machine template. The default value is set to \"vyos\".</p> </li> <li> <p><code>proxmox_addr</code> (string): Specifies the address of the Proxmox server.</p> </li> <li> <p><code>proxmox_node</code> (string): Specifies the Proxmox node where the virtual machine will be created.</p> </li> <li> <p><code>proxmox_username</code> (string): Specifies the username for authenticating with the Proxmox server.</p> </li> <li> <p><code>proxmox_password</code> (string): Specifies the password for authenticating with the Proxmox server.</p> </li> <li> <p><code>proxmox_token</code> (string): Specifies the Proxmox API token. The default value is obtained from the PACKER_PVE_TOKEN environment variable.</p> </li> <li> <p><code>vm_prefix</code> (string): Specifies a prefix for the virtual machine name.</p> </li> <li> <p><code>iso_storage</code> (string): Specifies the storage pool where the ISO image will be mounted.</p> </li> </ul>"},{"location":"projects/s3gw/auth-api/","title":"Auth API","text":""},{"location":"projects/s3gw/aws-s3-api/","title":"AWS S3 API","text":""},{"location":"projects/s3gw/s3gw/","title":"S3 Gateway","text":""},{"location":"projects/s3gw/s3gw/#getting-data-from-storage","title":"Getting data from storage","text":"<pre><code>sequenceDiagram\n    actor Client\n    participant S3 Gateway\n    participant PostgreSQL\n\n    Client-&gt;&gt;+S3 Gateway: GET /{bucket}/{key}\n        S3 Gateway-&gt;&gt;+PostgreSQL: is &lt;API_KEY&gt; authorized to &lt;/bucket&gt;?\n        PostgreSQL--&gt;&gt;-S3 Gateway: returns &lt;yes&gt;\n\n        S3 Gateway-&gt;&gt;+PostgreSQL: get &lt;data&gt; located in &lt;/bucket/key&gt; from storage\n        PostgreSQL--&gt;&gt;-S3 Gateway: returns &lt;data&gt;\n    S3 Gateway--&gt;&gt;-Client: returns &lt;data&gt;\n</code></pre>"},{"location":"projects/s3gw/s3gw/#creating-new-api-key","title":"Creating new API Key","text":"<pre><code>sequenceDiagram\n    actor Client\n    participant S3 Gateway\n    participant PostgreSQL\n\n    Client-&gt;&gt;+S3 Gateway: POST /auth/token\n        S3 Gateway-&gt;&gt;+PostgreSQL: generate and save &lt;API TOKEN&gt;\n        PostgreSQL--&gt;&gt;-S3 Gateway: returns &lt;API TOKEN&gt;\n    S3 Gateway--&gt;&gt;-Client: returns &lt;API TOKEN&gt;\n</code></pre>"},{"location":"terraform/maas-22.04/","title":"MaaS (Ubuntu 22.04)","text":""},{"location":"terraform/maas-22.04/#purpose","title":"Purpose","text":"<p>This Terraform file is used to provision multiple virtual machines (VMs) with Ubuntu 22.04 operating system on a Proxmox hypervisor using MAAS (Metal as a Service).</p>"},{"location":"terraform/maas-22.04/#variables","title":"Variables","text":"<ul> <li> <p><code>proxmox_addr</code> (string): The address of the Proxmox server.</p> </li> <li> <p><code>proxmox_node</code> (string): The name of the Proxmox node where the VMs will be created.</p> </li> <li> <p><code>proxmox_username</code> (string): The username used to authenticate with the Proxmox server.</p> </li> <li> <p><code>proxmox_password</code> (string): The password used to authenticate with the Proxmox server.</p> </li> <li> <p><code>storage</code> (string): The storage identifier where the VMs will be stored.</p> </li> <li> <p><code>os_minor_version</code> (number, default: 2): The minor version of Ubuntu 22.04 to be used for the VMs.</p> </li> <li> <p><code>replica_count</code> (number): Number of VMs to create.</p> </li> <li> <p><code>core_count</code> (number, default: 2): Number of cores per VM.</p> </li> <li> <p><code>memory_gib</code> (number, default: 4): Memory per VM in GiB.</p> </li> <li> <p><code>disk_size_gib</code> (number, default: 40): Disk size per VM in GiB.</p> </li> </ul>"},{"location":"terraform/rocky-9-k3s/","title":"Rocky 9 (K3s)","text":""},{"location":"terraform/rocky-9-k3s/#purpose","title":"Purpose","text":"<p>This Terraform file is used to provision multiple virtual machines (VMs) with Rocky Linux 9 operating system on a Proxmox hypervisor, specifically for running a Kubernetes cluster using K3s.</p>"},{"location":"terraform/rocky-9-k3s/#variables","title":"Variables","text":"<ul> <li> <p><code>proxmox_addr</code> (string): The address of the Proxmox server.</p> </li> <li> <p><code>proxmox_node</code> (string): The name of the Proxmox node where the VMs will be created.</p> </li> <li> <p><code>proxmox_username</code> (string): The username used to authenticate with the Proxmox server.</p> </li> <li> <p><code>proxmox_password</code> (string): The password used to authenticate with the Proxmox server.</p> </li> <li> <p><code>storage</code> (string): The storage identifier where the VMs will be stored.</p> </li> <li> <p><code>os_minor_version</code> (number, default: 2): The minor version of Rocky Linux 9 to be used for the VMs.</p> </li> <li> <p><code>replica_count</code> (number): Number of VMs to create.</p> </li> <li> <p><code>core_count</code> (number, default: 4): Number of cores per VM.</p> </li> <li> <p><code>memory_gib</code> (number, default: 8): Memory per VM in GiB.</p> </li> <li> <p><code>disk_size_gib</code> (number, default: 120): Disk size per VM in GiB.</p> </li> </ul>"},{"location":"terraform/rocky-9-lb/","title":"Rocky 9 (Load Balancer)","text":""},{"location":"terraform/rocky-9-lb/#purpose","title":"Purpose","text":"<p>This Terraform file is used to provision multiple virtual machines (VMs) with Rocky Linux 9 operating system on a Proxmox hypervisor, specifically for setting up a load balancer configuration.</p>"},{"location":"terraform/rocky-9-lb/#variables","title":"Variables","text":"<ul> <li> <p><code>proxmox_addr</code> (string): The address of the Proxmox server.</p> </li> <li> <p><code>proxmox_node</code> (string): The name of the Proxmox node where the VMs will be created.</p> </li> <li> <p><code>proxmox_username</code> (string): The username used to authenticate with the Proxmox server.</p> </li> <li> <p><code>proxmox_password</code> (string): The password used to authenticate with the Proxmox server.</p> </li> <li> <p><code>storage</code> (string): The storage identifier where the VMs will be stored.</p> </li> <li> <p><code>os_minor_version</code> (number, default: 2): The minor version of Rocky Linux 9 to be used for the VMs.</p> </li> <li> <p><code>replica_count</code> (number): Number of VMs to create.</p> </li> <li> <p><code>core_count</code> (number, default: 4): Number of cores per VM.</p> </li> <li> <p><code>memory_gib</code> (number, default: 4): Memory per VM in GiB.</p> </li> <li> <p><code>disk_size_gib</code> (number, default: 40): Disk size per VM in GiB.</p> </li> </ul>"},{"location":"terraform/rocky-9/","title":"Rocky 9","text":""},{"location":"terraform/rocky-9/#purpose","title":"Purpose","text":"<p>This Terraform file is used to provision multiple virtual machines (VMs) with Rocky Linux 9 operating system on a Proxmox hypervisor.</p>"},{"location":"terraform/rocky-9/#variables","title":"Variables","text":"<ul> <li> <p><code>proxmox_addr</code> (string): The address of the Proxmox server.</p> </li> <li> <p><code>proxmox_node</code> (string): The name of the Proxmox node where the VMs will be created.</p> </li> <li> <p><code>proxmox_username</code> (string): The username used to authenticate with the Proxmox server.</p> </li> <li> <p><code>proxmox_password</code> (string): The password used to authenticate with the Proxmox server.</p> </li> <li> <p><code>storage</code> (string): The storage identifier where the VMs will be stored.</p> </li> <li> <p><code>os_minor_version</code> (number, default: 2): The minor version of Rocky Linux 9 to be used for the VMs.</p> </li> <li> <p><code>replica_count</code> (number): Number of VMs to create.</p> </li> <li> <p><code>core_count</code> (number): Number of cores per VM.</p> </li> <li> <p><code>memory_gib</code> (number): Memory per VM in GiB.</p> </li> <li> <p><code>disk_size_gib</code> (number): Disk size per VM in GiB.</p> </li> </ul>"},{"location":"terraform/ubuntu-22.04/","title":"Ubuntu 22.04 LTS","text":""},{"location":"terraform/ubuntu-22.04/#purpose","title":"Purpose","text":"<p>This Terraform file is used to provision multiple virtual machines (VMs) with Ubuntu 22.04 operating system on a Proxmox hypervisor.</p>"},{"location":"terraform/ubuntu-22.04/#variables","title":"Variables","text":"<ul> <li> <p><code>proxmox_addr</code> (string): The address of the Proxmox server.</p> </li> <li> <p><code>proxmox_node</code> (string): The name of the Proxmox node where the VMs will be created.</p> </li> <li> <p><code>proxmox_username</code> (string): The username used to authenticate with the Proxmox server.</p> </li> <li> <p><code>proxmox_password</code> (string): The password used to authenticate with the Proxmox server.</p> </li> <li> <p><code>storage</code> (string): The storage identifier where the VMs will be stored.</p> </li> <li> <p><code>os_minor_version</code> (number, default: 2): The minor version of Ubuntu 22.04 to be used for the VMs.</p> </li> <li> <p><code>replica_count</code> (number): Number of VMs to create.</p> </li> <li> <p><code>core_count</code> (number): Number of cores per VM.</p> </li> <li> <p><code>memory_gib</code> (number): Memory per VM in GiB.</p> </li> <li> <p><code>disk_size_gib</code> (number): Disk size per VM in GiB.</p> </li> </ul>"}]}